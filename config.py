"""
Configuration file for Gemma-3N fine-tuning project.
Contains paths, model names, training parameters, and dataset settings.
"""

MODEL_NAME = "unsloth/gemma-3n-E4B-it"  # Base model
LOCAL_SAVE_PATH = "gemma-3n-chatapt"    # Where to save fine-tuned model

# GPU & memory settings
# CHANGED: Increased to 2048 to handle longer CTI report contexts
MAX_SEQ_LENGTH = 2048 
LOAD_IN_4BIT = True

# LoRA parameters
LORA_R = 16       # OPTIONAL: 16 is often better for domain adaptation than 8
LORA_ALPHA = 16   # OPTIONAL: Matching R is a safe standard
LORA_DROPOUT = 0
LORA_BIAS = "none"
RANDOM_STATE = 3407

# Dataset
# CHANGED: Points to your local file generated by data_prep.py
DATASET_NAME = "sharegpt_dataset.jsonl" 
# CHANGED: Local loading typically uses just "train" without slicing syntax
DATASET_SPLIT = "train" 
CHAT_TEMPLATE = "gemma" # "gemma" is the standard ID for Unsloth templates

# Training parameters
PER_DEVICE_BATCH_SIZE = 1
GRADIENT_ACCUMULATION_STEPS = 4
MAX_STEPS = 60
LEARNING_RATE = 2e-4
WARMUP_STEPS = 5
OPTIMIZER = "adamw_8bit"
WEIGHT_DECAY = 0.001
LR_SCHEDULER_TYPE = "linear"
REPORT_TO = "none"
SEED = 3407
